{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import timeit\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import models\n",
    "import tools as tl\n",
    "import tensorflow as tf\n",
    "import image_process as ip\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import newgenerator\n",
    "from keras_bcnn.models import BayesianUNet2D\n",
    "from tensorflow.keras.backend import categorical_crossentropy,binary_crossentropy\n",
    "from scipy.stats import entropy, norm\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Lambda,Reshape,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import gc\n",
    "from matplotlib import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#変数設定\n",
    "method = 'unet_patch'\n",
    "data = \"ips\"\n",
    "model_name = \"unet\"\n",
    "method = 'unet_patch'\n",
    "patch_size = 160\n",
    "batch_size = 12\n",
    "output_channel = 3\n",
    "mc_iteration=10\n",
    "reflect = False\n",
    "n_class=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    #Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def cce_dice_loss(y_true, y_pred):\n",
    "    loss = categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "  \n",
    "def original(y_true, y_pred):#重要　出力のエントロピーを0に近づけるloss\n",
    "    zero=tf.zeros([1,160,160])\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    e=-1*(y_pred[:,:,:,0]*tf.math.log(y_pred[:,:,:,0])+y_pred[:,:,:,1]*tf.math.log(y_pred[:,:,:,1])+y_pred[:,:,:,2]*tf.math.log(y_pred[:,:,:,2]))\n",
    "    e=tf.where(tf.math.is_nan(e), tf.zeros_like(e), e)\n",
    "    return mse(zero, e)\n",
    "\n",
    "def original2(y_true, y_pred):\n",
    "    tmp = tf.math.argmax(y_true, axis=3)\n",
    "    tmp2 = tf.math.argmax(y_pred, axis=3)\n",
    "    tmp3 = tf.math.subtract(tmp,tmp2)\n",
    "    tmp4 = tf.where(tmp3!=0,1.,0.)\n",
    "    e=-1*(y_pred[:,:,:,0]*tf.math.log(y_pred[:,:,:,0])+y_pred[:,:,:,1]*tf.math.log(y_pred[:,:,:,1])+y_pred[:,:,:,2]*tf.math.log(y_pred[:,:,:,2]))\n",
    "    e=tf.where(tf.math.is_nan(e), tf.zeros_like(e), e)\n",
    "    e = tf.clip_by_value(e, clip_value_max=1, clip_value_min=0)\n",
    "    #return categorical_crossentropy(y_true, y_pred) +dice_loss(tmp4, e)\n",
    "    return categorical_crossentropy(y_true, y_pred) +binary_crossentropy(tmp4, e)\n",
    "\n",
    "for mode in [\"test\"] :#\"train\",\"val\" \n",
    "    for data_num in [1,2,3,4,5]:\n",
    "        save_path = tl.get_save_path(data, method, 'unet',patch_size, data_num)\n",
    "        os.makedirs(save_path + '%s_un/uncertainty'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/uncertainty_T_0.5'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"%s\"%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"label\", exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/correctness'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/probability'%mode, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ytrue(mask_array, data):\n",
    "    #ips画像の赤を1、緑を2、青を3、紫を0にする関数\n",
    "        if data == \"ips\":\n",
    "            height, width, _ = mask_array.shape\n",
    "            good_label = ((mask_array[:,:,0] == 255)&\n",
    "                          (mask_array[:,:,1] == 0)&\n",
    "                          (mask_array[:,:,2] == 0)\n",
    "                         ) * np.ones((height, width)) * 1\n",
    "            bad_label = ((mask_array[:,:,0] == 0)&\n",
    "                         (mask_array[:,:,1] == 255)&\n",
    "                         (mask_array[:,:,2] == 0)\n",
    "                        ) * np.ones((height, width)) * 2\n",
    "            bgd_label = ((mask_array[:,:,0] == 0)&\n",
    "                         (mask_array[:,:,1] == 0)&\n",
    "                         (mask_array[:,:,2] == 255)\n",
    "                        ) * np.ones((height, width)) * 3\n",
    "            y_true = good_label + bad_label + bgd_label\n",
    "            return y_true\n",
    "        else:#melanomaの処理\n",
    "            height, width = mask_array.shape\n",
    "            y_true = (mask_array == 255) * np.ones((height, width))\n",
    "            return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_map(size, test_step, width, height, pre_map, ref_num, data, n_class):\n",
    "    #出力結果、出力結果を255で割った結果(評価に使う)、エントロピーマップ、確率マップを出力\n",
    "    print(\"Start visualize image...\")\n",
    "    num = 0\n",
    "    pre_label = np.zeros((height,width,n_class))\n",
    "    for y in range(0,height-size+1,test_step):\n",
    "        for x in range(0,width-size+1,test_step):\n",
    "            patch = np.ones((size,size,n_class))\n",
    "            patch = patch*pre_map[num]\n",
    "            pre_label[y:y+size,x:x+size,:] += patch\n",
    "            num += 1\n",
    "    sumaltion=np.sum(pre_label,axis=2)\n",
    "    pre_label[:,:,0]=pre_label[:,:,0]/sumaltion\n",
    "    pre_label[:,:,1]=pre_label[:,:,1]/sumaltion\n",
    "    pre_label[:,:,2]=pre_label[:,:,2]/sumaltion\n",
    "    e=entropy(pre_label,axis=2)\n",
    "    max=pre_label.max(axis=2)\n",
    "    pro = np.zeros((height,width,n_class))\n",
    "    pro1=pre_label[:,:,0]\n",
    "    pro2=pre_label[:,:,1]\n",
    "    pro3=pre_label[:,:,2]\n",
    "    pro1[pro1<max]=0\n",
    "    pro2[pro2<max]=0\n",
    "    pro3[pro3<max]=0\n",
    "    pro[:,:,0]=pro1\n",
    "    pro[:,:,1]=pro2\n",
    "    pro[:,:,2]=pro3\n",
    "    pro*=255   \n",
    "    if data in ['ips']:\n",
    "        vis_img, label_img = tl.get_InfImg(pre_label, data)\n",
    "    elif data in ['melanoma', 'colonoscopy_tissue']:\n",
    "        others = pre_label[:,:,1]\n",
    "        target = pre_label[:,:,0]\n",
    "        label_img = (target > others)*1\n",
    "        vis_img = label_img*255\n",
    "    return vis_img, label_img,e,pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2label(name, save_path):\n",
    "    mask = np.array(Image.open(save_path+\"/test/\" + name + '.png'), dtype=int)\n",
    "    label = mask / 255\n",
    "    if data == 'ips':\n",
    "        label = label[:, :, 0] * 1 + label[:, :, 1] * 2 + label[:, :, 2] * 3\n",
    "        label = Image.fromarray(np.uint8(label))\n",
    "        label.save(save_path + '/label/' + name + '.png')\n",
    "    else:\n",
    "        label = Image.fromarray(np.uint8(label))\n",
    "        label.save(save_path + '/label/' + name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_specificity(y_true, y_pred):#sensitivity_specificity loss\n",
    "    l=0.5\n",
    "    true_positives = K.sum(((y_true - y_pred)**2)*y_pred)\n",
    "    possible_positives = K.sum(K.clip(y_true, 0, 1))\n",
    "    \n",
    "    true_negatives = K.sum(((y_true - y_pred)**2)*(1-y_pred))\n",
    "    possible_negatives = K.sum(K.clip(1-y_true, 0, 1))\n",
    "    return l*(true_positives / (possible_positives + K.epsilon()))+(1-l)*(true_negatives / (possible_negatives + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n",
      "memory growth: True\n",
      "dataset_1 start!!\n",
      "WARNING:tensorflow:From <ipython-input-8-e7912cc1f89b>:35: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "   2/2485 [..............................] - ETA: 2:08 - loss: 0.5578 - accuracy: 0.3802WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0384s vs `on_train_batch_end` time: 0.0629s). Check your callbacks.\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.4032"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sora/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Training option is ignored..\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36396, saving model to ./ips/unet_patch/unet/size_160/dataset_1/weights/weights.h5\n",
      "2485/2485 [==============================] - 910s 366ms/step - loss: 0.4428 - accuracy: 0.4032 - val_loss: 0.3640 - val_accuracy: 0.5111\n",
      "Epoch 2/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.4020\n",
      "Epoch 00002: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 287s 116ms/step - loss: 0.4509 - accuracy: 0.4020 - val_loss: 0.3985 - val_accuracy: 0.4699\n",
      "Epoch 3/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.3977\n",
      "Epoch 00003: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 289s 116ms/step - loss: 0.4562 - accuracy: 0.3977 - val_loss: 0.3967 - val_accuracy: 0.4711\n",
      "Epoch 4/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.3765\n",
      "Epoch 00004: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 289s 116ms/step - loss: 0.5006 - accuracy: 0.3765 - val_loss: 0.4888 - val_accuracy: 0.3961\n",
      "Epoch 5/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.3628\n",
      "Epoch 00005: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 289s 116ms/step - loss: 0.5285 - accuracy: 0.3628 - val_loss: 0.5280 - val_accuracy: 0.3791\n",
      "Epoch 6/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.3758\n",
      "Epoch 00006: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 289s 116ms/step - loss: 0.4960 - accuracy: 0.3758 - val_loss: 0.4022 - val_accuracy: 0.4570\n",
      "Epoch 7/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.3938\n",
      "Epoch 00007: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 290s 117ms/step - loss: 0.4541 - accuracy: 0.3938 - val_loss: 0.4094 - val_accuracy: 0.4580\n",
      "Epoch 8/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.3971\n",
      "Epoch 00008: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 290s 117ms/step - loss: 0.4551 - accuracy: 0.3971 - val_loss: 0.3770 - val_accuracy: 0.4857\n",
      "Epoch 9/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.4006\n",
      "Epoch 00009: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 288s 116ms/step - loss: 0.4473 - accuracy: 0.4006 - val_loss: 0.3921 - val_accuracy: 0.4668\n",
      "Epoch 10/10\n",
      "2485/2485 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.3886\n",
      "Epoch 00010: val_loss did not improve from 0.36396\n",
      "2485/2485 [==============================] - 289s 116ms/step - loss: 0.4507 - accuracy: 0.3886 - val_loss: 0.3897 - val_accuracy: 0.4429\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "dataset_2 start!!\n",
      "Epoch 1/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.4025\n",
      "Epoch 00001: val_loss improved from inf to 0.41672, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 863s 334ms/step - loss: 0.4461 - accuracy: 0.4025 - val_loss: 0.4167 - val_accuracy: 0.4406\n",
      "Epoch 2/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.3981\n",
      "Epoch 00002: val_loss did not improve from 0.41672\n",
      "2586/2586 [==============================] - 295s 114ms/step - loss: 0.4461 - accuracy: 0.3981 - val_loss: 0.4485 - val_accuracy: 0.3754\n",
      "Epoch 3/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.4004\n",
      "Epoch 00003: val_loss improved from 0.41672 to 0.41561, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 298s 115ms/step - loss: 0.4459 - accuracy: 0.4004 - val_loss: 0.4156 - val_accuracy: 0.4435\n",
      "Epoch 4/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4027\n",
      "Epoch 00004: val_loss did not improve from 0.41561\n",
      "2586/2586 [==============================] - 296s 115ms/step - loss: 0.4465 - accuracy: 0.4027 - val_loss: 0.4156 - val_accuracy: 0.4435\n",
      "Epoch 5/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4027\n",
      "Epoch 00005: val_loss improved from 0.41561 to 0.41524, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 298s 115ms/step - loss: 0.4465 - accuracy: 0.4027 - val_loss: 0.4152 - val_accuracy: 0.4445\n",
      "Epoch 6/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4028\n",
      "Epoch 00006: val_loss improved from 0.41524 to 0.41521, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 298s 115ms/step - loss: 0.4465 - accuracy: 0.4028 - val_loss: 0.4152 - val_accuracy: 0.4445\n",
      "Epoch 7/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4029\n",
      "Epoch 00007: val_loss improved from 0.41521 to 0.41516, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 298s 115ms/step - loss: 0.4465 - accuracy: 0.4029 - val_loss: 0.4152 - val_accuracy: 0.4447\n",
      "Epoch 8/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4029\n",
      "Epoch 00008: val_loss improved from 0.41516 to 0.41515, saving model to ./ips/unet_patch/unet/size_160/dataset_2/weights/weights.h5\n",
      "2586/2586 [==============================] - 298s 115ms/step - loss: 0.4465 - accuracy: 0.4029 - val_loss: 0.4152 - val_accuracy: 0.4447\n",
      "Epoch 9/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4029\n",
      "Epoch 00009: val_loss did not improve from 0.41515\n",
      "2586/2586 [==============================] - 296s 114ms/step - loss: 0.4465 - accuracy: 0.4029 - val_loss: 0.4152 - val_accuracy: 0.4447\n",
      "Epoch 10/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.4029\n",
      "Epoch 00010: val_loss did not improve from 0.41515\n",
      "2586/2586 [==============================] - 296s 114ms/step - loss: 0.4465 - accuracy: 0.4029 - val_loss: 0.4152 - val_accuracy: 0.4447\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "dataset_3 start!!\n",
      "Epoch 1/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.4328\n",
      "Epoch 00001: val_loss improved from inf to 0.44142, saving model to ./ips/unet_patch/unet/size_160/dataset_3/weights/weights.h5\n",
      "2650/2650 [==============================] - 956s 361ms/step - loss: 0.4212 - accuracy: 0.4328 - val_loss: 0.4414 - val_accuracy: 0.4093\n",
      "Epoch 2/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.4346\n",
      "Epoch 00002: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 533s 201ms/step - loss: 0.4198 - accuracy: 0.4346 - val_loss: 0.4418 - val_accuracy: 0.4091\n",
      "Epoch 3/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.4372\n",
      "Epoch 00003: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 937s 354ms/step - loss: 0.4207 - accuracy: 0.4372 - val_loss: 0.4418 - val_accuracy: 0.4092\n",
      "Epoch 4/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.4366\n",
      "Epoch 00004: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 342s 129ms/step - loss: 0.4212 - accuracy: 0.4366 - val_loss: 0.4430 - val_accuracy: 0.4084\n",
      "Epoch 5/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.4369\n",
      "Epoch 00005: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4210 - accuracy: 0.4369 - val_loss: 0.4477 - val_accuracy: 0.4055\n",
      "Epoch 6/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.4237\n",
      "Epoch 00006: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4352 - accuracy: 0.4237 - val_loss: 0.4501 - val_accuracy: 0.4047\n",
      "Epoch 7/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.4242\n",
      "Epoch 00007: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4356 - accuracy: 0.4242 - val_loss: 0.4816 - val_accuracy: 0.3987\n",
      "Epoch 8/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.4024\n",
      "Epoch 00008: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4867 - accuracy: 0.4024 - val_loss: 0.4483 - val_accuracy: 0.4067\n",
      "Epoch 9/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.4255\n",
      "Epoch 00009: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4353 - accuracy: 0.4255 - val_loss: 0.4634 - val_accuracy: 0.4025\n",
      "Epoch 10/10\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.4498 - accuracy: 0.4173\n",
      "Epoch 00010: val_loss did not improve from 0.44142\n",
      "2650/2650 [==============================] - 304s 115ms/step - loss: 0.4498 - accuracy: 0.4173 - val_loss: 0.4740 - val_accuracy: 0.4005\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "dataset_4 start!!\n",
      "Epoch 1/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.4566\n",
      "Epoch 00001: val_loss improved from inf to 0.46193, saving model to ./ips/unet_patch/unet/size_160/dataset_4/weights/weights.h5\n",
      "2575/2575 [==============================] - 1064s 413ms/step - loss: 0.4059 - accuracy: 0.4566 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 2/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00002: val_loss improved from 0.46193 to 0.46192, saving model to ./ips/unet_patch/unet/size_160/dataset_4/weights/weights.h5\n",
      "2575/2575 [==============================] - 298s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 3/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00003: val_loss improved from 0.46192 to 0.46192, saving model to ./ips/unet_patch/unet/size_160/dataset_4/weights/weights.h5\n",
      "2575/2575 [==============================] - 300s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 4/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.4589\n",
      "Epoch 00004: val_loss improved from 0.46192 to 0.46192, saving model to ./ips/unet_patch/unet/size_160/dataset_4/weights/weights.h5\n",
      "2575/2575 [==============================] - 299s 116ms/step - loss: 0.4046 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 5/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00005: val_loss did not improve from 0.46192\n",
      "2575/2575 [==============================] - 298s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 6/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00006: val_loss did not improve from 0.46192\n",
      "2575/2575 [==============================] - 680s 264ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 7/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00007: val_loss did not improve from 0.46192\n",
      "2575/2575 [==============================] - 701s 272ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 8/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00008: val_loss did not improve from 0.46192\n",
      "2575/2575 [==============================] - 298s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 9/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00009: val_loss improved from 0.46192 to 0.46192, saving model to ./ips/unet_patch/unet/size_160/dataset_4/weights/weights.h5\n",
      "2575/2575 [==============================] - 299s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "Epoch 10/10\n",
      "2575/2575 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.4589\n",
      "Epoch 00010: val_loss did not improve from 0.46192\n",
      "2575/2575 [==============================] - 298s 116ms/step - loss: 0.4045 - accuracy: 0.4589 - val_loss: 0.4619 - val_accuracy: 0.3822\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "dataset_5 start!!\n",
      "Epoch 1/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.4564\n",
      "Epoch 00001: val_loss improved from inf to 0.43611, saving model to ./ips/unet_patch/unet/size_160/dataset_5/weights/weights.h5\n",
      "2510/2510 [==============================] - 887s 353ms/step - loss: 0.4065 - accuracy: 0.4564 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 2/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00002: val_loss improved from 0.43611 to 0.43611, saving model to ./ips/unet_patch/unet/size_160/dataset_5/weights/weights.h5\n",
      "2510/2510 [==============================] - 735s 293ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 3/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00003: val_loss improved from 0.43611 to 0.43611, saving model to ./ips/unet_patch/unet/size_160/dataset_5/weights/weights.h5\n",
      "2510/2510 [==============================] - 294s 117ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 4/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00004: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 292s 116ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 5/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00005: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 640s 255ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 6/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00006: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 513s 204ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 7/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00007: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 292s 116ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 8/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00008: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 292s 116ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 9/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00009: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 292s 116ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n",
      "Epoch 10/10\n",
      "2510/2510 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.4576\n",
      "Epoch 00010: val_loss did not improve from 0.43611\n",
      "2510/2510 [==============================] - 292s 116ms/step - loss: 0.4055 - accuracy: 0.4576 - val_loss: 0.4361 - val_accuracy: 0.4169\n"
     ]
    }
   ],
   "source": [
    "#一旦categorical_crossentropyで学習する.\n",
    "for data_num in [1,2,3,4,5]:\n",
    "    epochs = 5\n",
    "    clear_session()\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        for k in range(len(physical_devices)):\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[k], True)\n",
    "            print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[k]))\n",
    "    else:\n",
    "        print(\"Not enough GPU hardware devices available\")\n",
    "    save_path = tl.get_save_path(data, method, model_name,patch_size, data_num)\n",
    "    model = BayesianUNet2D((patch_size, patch_size, 3), output_channel).build()\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    tl.make_dirs(data, method, model_name,patch_size, data_num)\n",
    "    \n",
    "    train=\"/home/sora/new_project/crop/dataset_%d/train/\"%data_num\n",
    "    val=\"/home/sora/new_project/crop/dataset_%d/val/\"%data_num\n",
    "    # train -----------------------------------------------------------\n",
    "    train_gen = newgenerator.ImageSequence(train,batch_size,\"train\",data_num)\n",
    "    valid_gen = newgenerator.ImageSequence(val, batch_size,\"val\",data_num)\n",
    "    print(\"dataset_%d start!!\"%data_num)\n",
    "    os.makedirs(save_path + 'weights', exist_ok=True)\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(save_path,'weights', 'weights.h5'),\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "    history = model.fit_generator(generator=train_gen,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_gen),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint],\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=len(valid_gen))\n",
    "\n",
    "    tl.draw_train_loss_plot(history, save_path)\n",
    "    del model,history,train_gen,valid_gen,model_checkpoint,save_path\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_train_loss_plot2(history, save_path):\n",
    "    v_l = history.history[\"val_loss\"]\n",
    "    vv = history.history[\"val_lambda_2_loss\"]\n",
    "    vvv = history.history[\"val_lambda_2_1_loss\"]\n",
    "    nb_epoch = len(v_l)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"epoch\")  # x軸ラベル\n",
    "    ax.set_ylabel(\"loss\")  # y軸ラベル\n",
    "    # ax.set_aspect('equal') # スケールを揃える\n",
    "    ax.grid()            # 罫線\n",
    "    ax.plot(range(nb_epoch), v_l, label=\"val_loss\")\n",
    "    ax.plot(range(nb_epoch), vv, label=\"val_CE_loss\")\n",
    "    ax.plot(range(nb_epoch), vvv, label=\"val_MSE_loss\")\n",
    "    fig.tight_layout()  # レイアウトの設定\n",
    "    ax.legend([\"val_loss\",\"val_CE_loss\",\"val_MSE_loss\"],loc='best', fontsize=10)\n",
    "    plt.savefig(save_path + \"loss2.png\")\n",
    "    plt.close()\n",
    "def draw_train_loss_plot3(history, save_path):\n",
    "    v_l = history.history[\"lambda_2_loss\"]\n",
    "    vv = history.history[\"val_lambda_2_loss\"]\n",
    "    #vvv = history.history[\"val_lambda_5_1_loss\"]\n",
    "    nb_epoch = len(v_l)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"epoch\")  # x軸ラベル\n",
    "    ax.set_ylabel(\"loss\")  # y軸ラベル\n",
    "    # ax.set_aspect('equal') # スケールを揃える\n",
    "    ax.grid()            # 罫線\n",
    "    ax.plot(range(nb_epoch), v_l, label=\"CE_loss\")\n",
    "    ax.plot(range(nb_epoch), vv, label=\"val_CE_loss\")\n",
    "    #ax.plot(range(nb_epoch), vvv, label=\"val_MSE_loss\")\n",
    "    fig.tight_layout()  # レイアウトの設定\n",
    "    ax.legend([\"CE_loss\",\"val_CE_loss\"],loc='best', fontsize=10)\n",
    "    plt.savefig(save_path + \"loss3.png\")\n",
    "    plt.close()\n",
    "def draw_train_loss_plot4(history, save_path):\n",
    "    v_l = history.history[\"lambda_2_1_loss\"]\n",
    "    vv = history.history[\"val_lambda_2_1_loss\"]\n",
    "    #vvv = history.history[\"val_lambda_5_1_loss\"]\n",
    "    nb_epoch = len(v_l)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"epoch\")  # x軸ラベル\n",
    "    ax.set_ylabel(\"loss\")  # y軸ラベル\n",
    "    # ax.set_aspect('equal') # スケールを揃える\n",
    "    ax.grid()            # 罫線\n",
    "    ax.plot(range(nb_epoch), v_l, label=\"MSE_loss\")\n",
    "    ax.plot(range(nb_epoch), vv, label=\"val_NSE_loss\")\n",
    "    #ax.plot(range(nb_epoch), vvv, label=\"val_MSE_loss\")\n",
    "    fig.tight_layout()  # レイアウトの設定\n",
    "    ax.legend([\"MSE_loss\",\"val_MSE_loss\"],loc='best', fontsize=10)\n",
    "    plt.savefig(save_path + \"loss4.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n",
      "memory growth: True\n",
      "WARNING:tensorflow:From <ipython-input-9-a0bf82d23201>:54: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1784 - lambda_2_loss: 0.1396 - lambda_2_1_loss: 0.0388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sora/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Training option is ignored..\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42696, saving model to ./ips/unet_patch/unet/size_160/dataset_2/stage2/weights2/weights.h5\n",
      "34002/34002 [==============================] - 3371s 99ms/step - loss: 0.1784 - lambda_2_loss: 0.1396 - lambda_2_1_loss: 0.0388 - val_loss: 0.4270 - val_lambda_2_loss: 0.3689 - val_lambda_2_1_loss: 0.0580\n",
      "Epoch 2/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1667 - lambda_2_loss: 0.1319 - lambda_2_1_loss: 0.0348\n",
      "Epoch 00002: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3346s 98ms/step - loss: 0.1667 - lambda_2_loss: 0.1319 - lambda_2_1_loss: 0.0348 - val_loss: 0.4628 - val_lambda_2_loss: 0.4065 - val_lambda_2_1_loss: 0.0562\n",
      "Epoch 3/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1587 - lambda_2_loss: 0.1259 - lambda_2_1_loss: 0.0328\n",
      "Epoch 00003: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3350s 99ms/step - loss: 0.1587 - lambda_2_loss: 0.1259 - lambda_2_1_loss: 0.0328 - val_loss: 0.4394 - val_lambda_2_loss: 0.3861 - val_lambda_2_1_loss: 0.0533\n",
      "Epoch 4/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1520 - lambda_2_loss: 0.1207 - lambda_2_1_loss: 0.0312\n",
      "Epoch 00004: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3345s 98ms/step - loss: 0.1520 - lambda_2_loss: 0.1207 - lambda_2_1_loss: 0.0312 - val_loss: 0.4754 - val_lambda_2_loss: 0.4201 - val_lambda_2_1_loss: 0.0553\n",
      "Epoch 5/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1458 - lambda_2_loss: 0.1158 - lambda_2_1_loss: 0.0300\n",
      "Epoch 00005: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3337s 98ms/step - loss: 0.1458 - lambda_2_loss: 0.1158 - lambda_2_1_loss: 0.0300 - val_loss: 0.4779 - val_lambda_2_loss: 0.4238 - val_lambda_2_1_loss: 0.0541\n",
      "Epoch 6/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1400 - lambda_2_loss: 0.1112 - lambda_2_1_loss: 0.0288\n",
      "Epoch 00006: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3341s 98ms/step - loss: 0.1400 - lambda_2_loss: 0.1112 - lambda_2_1_loss: 0.0288 - val_loss: 0.4891 - val_lambda_2_loss: 0.4351 - val_lambda_2_1_loss: 0.0540\n",
      "Epoch 7/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1345 - lambda_2_loss: 0.1066 - lambda_2_1_loss: 0.0279\n",
      "Epoch 00007: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3345s 98ms/step - loss: 0.1345 - lambda_2_loss: 0.1066 - lambda_2_1_loss: 0.0279 - val_loss: 0.4899 - val_lambda_2_loss: 0.4364 - val_lambda_2_1_loss: 0.0535\n",
      "Epoch 8/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1295 - lambda_2_loss: 0.1025 - lambda_2_1_loss: 0.0271\n",
      "Epoch 00008: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3340s 98ms/step - loss: 0.1295 - lambda_2_loss: 0.1025 - lambda_2_1_loss: 0.0271 - val_loss: 0.5079 - val_lambda_2_loss: 0.4551 - val_lambda_2_1_loss: 0.0528\n",
      "Epoch 9/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1245 - lambda_2_loss: 0.0982 - lambda_2_1_loss: 0.0262\n",
      "Epoch 00009: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3335s 98ms/step - loss: 0.1245 - lambda_2_loss: 0.0982 - lambda_2_1_loss: 0.0262 - val_loss: 0.5176 - val_lambda_2_loss: 0.4638 - val_lambda_2_1_loss: 0.0539\n",
      "Epoch 10/10\n",
      "34002/34002 [==============================] - ETA: 0s - loss: 0.1199 - lambda_2_loss: 0.0942 - lambda_2_1_loss: 0.0256\n",
      "Epoch 00010: val_loss did not improve from 0.42696\n",
      "34002/34002 [==============================] - 3332s 98ms/step - loss: 0.1199 - lambda_2_loss: 0.0942 - lambda_2_1_loss: 0.0256 - val_loss: 0.5526 - val_lambda_2_loss: 0.4978 - val_lambda_2_1_loss: 0.0549\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.2022 - lambda_2_loss: 0.1596 - lambda_2_1_loss: 0.0426\n",
      "Epoch 00001: val_loss improved from inf to 0.26659, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights2/weights.h5\n",
      "35025/35025 [==============================] - 3565s 102ms/step - loss: 0.2022 - lambda_2_loss: 0.1596 - lambda_2_1_loss: 0.0426 - val_loss: 0.2666 - val_lambda_2_loss: 0.2334 - val_lambda_2_1_loss: 0.0332\n",
      "Epoch 2/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1921 - lambda_2_loss: 0.1517 - lambda_2_1_loss: 0.0404\n",
      "Epoch 00002: val_loss did not improve from 0.26659\n",
      "35025/35025 [==============================] - 3463s 99ms/step - loss: 0.1921 - lambda_2_loss: 0.1517 - lambda_2_1_loss: 0.0404 - val_loss: 0.2757 - val_lambda_2_loss: 0.2422 - val_lambda_2_1_loss: 0.0335\n",
      "Epoch 3/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1832 - lambda_2_loss: 0.1446 - lambda_2_1_loss: 0.0386\n",
      "Epoch 00003: val_loss did not improve from 0.26659\n",
      "35025/35025 [==============================] - 3468s 99ms/step - loss: 0.1832 - lambda_2_loss: 0.1446 - lambda_2_1_loss: 0.0386 - val_loss: 0.2684 - val_lambda_2_loss: 0.2335 - val_lambda_2_1_loss: 0.0349\n",
      "Epoch 4/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1753 - lambda_2_loss: 0.1380 - lambda_2_1_loss: 0.0372\n",
      "Epoch 00004: val_loss did not improve from 0.26659\n",
      "35025/35025 [==============================] - 3456s 99ms/step - loss: 0.1753 - lambda_2_loss: 0.1380 - lambda_2_1_loss: 0.0372 - val_loss: 0.2701 - val_lambda_2_loss: 0.2371 - val_lambda_2_1_loss: 0.0330\n",
      "Epoch 5/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1678 - lambda_2_loss: 0.1318 - lambda_2_1_loss: 0.0360\n",
      "Epoch 00005: val_loss improved from 0.26659 to 0.26642, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights2/weights.h5\n",
      "35025/35025 [==============================] - 3443s 98ms/step - loss: 0.1678 - lambda_2_loss: 0.1318 - lambda_2_1_loss: 0.0360 - val_loss: 0.2664 - val_lambda_2_loss: 0.2328 - val_lambda_2_1_loss: 0.0336\n",
      "Epoch 6/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1610 - lambda_2_loss: 0.1261 - lambda_2_1_loss: 0.0349\n",
      "Epoch 00006: val_loss did not improve from 0.26642\n",
      "35025/35025 [==============================] - 3440s 98ms/step - loss: 0.1610 - lambda_2_loss: 0.1261 - lambda_2_1_loss: 0.0349 - val_loss: 0.2745 - val_lambda_2_loss: 0.2407 - val_lambda_2_1_loss: 0.0338\n",
      "Epoch 7/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1548 - lambda_2_loss: 0.1207 - lambda_2_1_loss: 0.0341\n",
      "Epoch 00007: val_loss did not improve from 0.26642\n",
      "35025/35025 [==============================] - 3439s 98ms/step - loss: 0.1548 - lambda_2_loss: 0.1207 - lambda_2_1_loss: 0.0341 - val_loss: 0.2802 - val_lambda_2_loss: 0.2454 - val_lambda_2_1_loss: 0.0349\n",
      "Epoch 8/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1488 - lambda_2_loss: 0.1155 - lambda_2_1_loss: 0.0333\n",
      "Epoch 00008: val_loss did not improve from 0.26642\n",
      "35025/35025 [==============================] - 3437s 98ms/step - loss: 0.1488 - lambda_2_loss: 0.1155 - lambda_2_1_loss: 0.0333 - val_loss: 0.2797 - val_lambda_2_loss: 0.2447 - val_lambda_2_1_loss: 0.0350\n",
      "Epoch 9/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1432 - lambda_2_loss: 0.1106 - lambda_2_1_loss: 0.0326\n",
      "Epoch 00009: val_loss did not improve from 0.26642\n",
      "35025/35025 [==============================] - 3435s 98ms/step - loss: 0.1432 - lambda_2_loss: 0.1106 - lambda_2_1_loss: 0.0326 - val_loss: 0.2968 - val_lambda_2_loss: 0.2636 - val_lambda_2_1_loss: 0.0332\n",
      "Epoch 10/10\n",
      "35025/35025 [==============================] - ETA: 0s - loss: 0.1376 - lambda_2_loss: 0.1058 - lambda_2_1_loss: 0.0317\n",
      "Epoch 00010: val_loss did not improve from 0.26642\n",
      "35025/35025 [==============================] - 3439s 98ms/step - loss: 0.1376 - lambda_2_loss: 0.1058 - lambda_2_1_loss: 0.0317 - val_loss: 0.2812 - val_lambda_2_loss: 0.2481 - val_lambda_2_1_loss: 0.0331\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2852 - lambda_2_loss: 0.2218 - lambda_2_1_loss: 0.0634\n",
      "Epoch 00001: val_loss improved from inf to 0.21814, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights2/weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36632/36632 [==============================] - 3646s 100ms/step - loss: 0.2852 - lambda_2_loss: 0.2218 - lambda_2_1_loss: 0.0634 - val_loss: 0.2181 - val_lambda_2_loss: 0.1792 - val_lambda_2_1_loss: 0.0390\n",
      "Epoch 2/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2623 - lambda_2_loss: 0.2059 - lambda_2_1_loss: 0.0564\n",
      "Epoch 00002: val_loss improved from 0.21814 to 0.21808, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights2/weights.h5\n",
      "36632/36632 [==============================] - 3606s 98ms/step - loss: 0.2623 - lambda_2_loss: 0.2059 - lambda_2_1_loss: 0.0564 - val_loss: 0.2181 - val_lambda_2_loss: 0.1820 - val_lambda_2_1_loss: 0.0361\n",
      "Epoch 3/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2463 - lambda_2_loss: 0.1942 - lambda_2_1_loss: 0.0520\n",
      "Epoch 00003: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3595s 98ms/step - loss: 0.2463 - lambda_2_loss: 0.1942 - lambda_2_1_loss: 0.0520 - val_loss: 0.2306 - val_lambda_2_loss: 0.1954 - val_lambda_2_1_loss: 0.0352\n",
      "Epoch 4/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2329 - lambda_2_loss: 0.1838 - lambda_2_1_loss: 0.0491\n",
      "Epoch 00004: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3604s 98ms/step - loss: 0.2329 - lambda_2_loss: 0.1838 - lambda_2_1_loss: 0.0491 - val_loss: 0.2217 - val_lambda_2_loss: 0.1881 - val_lambda_2_1_loss: 0.0336\n",
      "Epoch 5/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2201 - lambda_2_loss: 0.1736 - lambda_2_1_loss: 0.0466\n",
      "Epoch 00005: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3618s 99ms/step - loss: 0.2201 - lambda_2_loss: 0.1736 - lambda_2_1_loss: 0.0466 - val_loss: 0.2294 - val_lambda_2_loss: 0.1946 - val_lambda_2_1_loss: 0.0348\n",
      "Epoch 6/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.2092 - lambda_2_loss: 0.1645 - lambda_2_1_loss: 0.0447\n",
      "Epoch 00006: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3624s 99ms/step - loss: 0.2092 - lambda_2_loss: 0.1645 - lambda_2_1_loss: 0.0447 - val_loss: 0.2321 - val_lambda_2_loss: 0.1964 - val_lambda_2_1_loss: 0.0356\n",
      "Epoch 7/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.1990 - lambda_2_loss: 0.1560 - lambda_2_1_loss: 0.0429\n",
      "Epoch 00007: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3603s 98ms/step - loss: 0.1990 - lambda_2_loss: 0.1560 - lambda_2_1_loss: 0.0429 - val_loss: 0.2353 - val_lambda_2_loss: 0.2008 - val_lambda_2_1_loss: 0.0345\n",
      "Epoch 8/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.1897 - lambda_2_loss: 0.1484 - lambda_2_1_loss: 0.0414\n",
      "Epoch 00008: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3627s 99ms/step - loss: 0.1897 - lambda_2_loss: 0.1484 - lambda_2_1_loss: 0.0414 - val_loss: 0.2270 - val_lambda_2_loss: 0.1942 - val_lambda_2_1_loss: 0.0327\n",
      "Epoch 9/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.1814 - lambda_2_loss: 0.1415 - lambda_2_1_loss: 0.0399\n",
      "Epoch 00009: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3628s 99ms/step - loss: 0.1814 - lambda_2_loss: 0.1415 - lambda_2_1_loss: 0.0399 - val_loss: 0.2321 - val_lambda_2_loss: 0.1972 - val_lambda_2_1_loss: 0.0349\n",
      "Epoch 10/10\n",
      "36632/36632 [==============================] - ETA: 0s - loss: 0.1740 - lambda_2_loss: 0.1351 - lambda_2_1_loss: 0.0389\n",
      "Epoch 00010: val_loss did not improve from 0.21808\n",
      "36632/36632 [==============================] - 3606s 98ms/step - loss: 0.1740 - lambda_2_loss: 0.1351 - lambda_2_1_loss: 0.0389 - val_loss: 0.2393 - val_lambda_2_loss: 0.2056 - val_lambda_2_1_loss: 0.0336\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.2265 - lambda_2_loss: 0.1773 - lambda_2_1_loss: 0.0491\n",
      "Epoch 00001: val_loss improved from inf to 0.26599, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights2/weights.h5\n",
      "33682/33682 [==============================] - 3364s 100ms/step - loss: 0.2265 - lambda_2_loss: 0.1773 - lambda_2_1_loss: 0.0491 - val_loss: 0.2660 - val_lambda_2_loss: 0.2297 - val_lambda_2_1_loss: 0.0363\n",
      "Epoch 2/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.2127 - lambda_2_loss: 0.1674 - lambda_2_1_loss: 0.0453\n",
      "Epoch 00002: val_loss improved from 0.26599 to 0.25168, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights2/weights.h5\n",
      "33682/33682 [==============================] - 3336s 99ms/step - loss: 0.2127 - lambda_2_loss: 0.1674 - lambda_2_1_loss: 0.0453 - val_loss: 0.2517 - val_lambda_2_loss: 0.2166 - val_lambda_2_1_loss: 0.0351\n",
      "Epoch 3/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.2025 - lambda_2_loss: 0.1595 - lambda_2_1_loss: 0.0429\n",
      "Epoch 00003: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3341s 99ms/step - loss: 0.2025 - lambda_2_loss: 0.1595 - lambda_2_1_loss: 0.0429 - val_loss: 0.2679 - val_lambda_2_loss: 0.2340 - val_lambda_2_1_loss: 0.0339\n",
      "Epoch 4/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1930 - lambda_2_loss: 0.1522 - lambda_2_1_loss: 0.0409\n",
      "Epoch 00004: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3338s 99ms/step - loss: 0.1930 - lambda_2_loss: 0.1522 - lambda_2_1_loss: 0.0409 - val_loss: 0.2634 - val_lambda_2_loss: 0.2302 - val_lambda_2_1_loss: 0.0332\n",
      "Epoch 5/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1849 - lambda_2_loss: 0.1455 - lambda_2_1_loss: 0.0394\n",
      "Epoch 00005: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3333s 99ms/step - loss: 0.1849 - lambda_2_loss: 0.1455 - lambda_2_1_loss: 0.0394 - val_loss: 0.2683 - val_lambda_2_loss: 0.2362 - val_lambda_2_1_loss: 0.0321\n",
      "Epoch 6/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1774 - lambda_2_loss: 0.1393 - lambda_2_1_loss: 0.0381\n",
      "Epoch 00006: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3335s 99ms/step - loss: 0.1774 - lambda_2_loss: 0.1393 - lambda_2_1_loss: 0.0381 - val_loss: 0.2769 - val_lambda_2_loss: 0.2450 - val_lambda_2_1_loss: 0.0319\n",
      "Epoch 7/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1701 - lambda_2_loss: 0.1332 - lambda_2_1_loss: 0.0369\n",
      "Epoch 00007: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3339s 99ms/step - loss: 0.1701 - lambda_2_loss: 0.1332 - lambda_2_1_loss: 0.0369 - val_loss: 0.2919 - val_lambda_2_loss: 0.2597 - val_lambda_2_1_loss: 0.0322\n",
      "Epoch 8/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1637 - lambda_2_loss: 0.1278 - lambda_2_1_loss: 0.0359\n",
      "Epoch 00008: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3341s 99ms/step - loss: 0.1637 - lambda_2_loss: 0.1278 - lambda_2_1_loss: 0.0359 - val_loss: 0.2915 - val_lambda_2_loss: 0.2585 - val_lambda_2_1_loss: 0.0330\n",
      "Epoch 9/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1575 - lambda_2_loss: 0.1224 - lambda_2_1_loss: 0.0351\n",
      "Epoch 00009: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3329s 99ms/step - loss: 0.1575 - lambda_2_loss: 0.1224 - lambda_2_1_loss: 0.0351 - val_loss: 0.2917 - val_lambda_2_loss: 0.2589 - val_lambda_2_1_loss: 0.0328\n",
      "Epoch 10/10\n",
      "33682/33682 [==============================] - ETA: 0s - loss: 0.1519 - lambda_2_loss: 0.1177 - lambda_2_1_loss: 0.0342\n",
      "Epoch 00010: val_loss did not improve from 0.25168\n",
      "33682/33682 [==============================] - 3328s 99ms/step - loss: 0.1519 - lambda_2_loss: 0.1177 - lambda_2_1_loss: 0.0342 - val_loss: 0.3089 - val_lambda_2_loss: 0.2756 - val_lambda_2_1_loss: 0.0332\n"
     ]
    }
   ],
   "source": [
    "#自作lossで学習\n",
    "for data_num in [2,3,4,5]:\n",
    "    clear_session()\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        for k in range(len(physical_devices)):\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[k], True)\n",
    "            print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[k]))\n",
    "    else:\n",
    "        print(\"Not enough GPU hardware devices available\")\n",
    "        \n",
    "    save_path = tl.get_save_path(data, method, model_name,patch_size, data_num)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 1\n",
    "    \n",
    "    model = BayesianUNet2D((patch_size, patch_size, 3), output_channel).build()\n",
    "    model.load_weights(save_path+'weights/weights.h5')\n",
    "    input_shape = (patch_size,patch_size,output_channel)\n",
    "    inputs2 = Input(input_shape)\n",
    "    mc_samples = Lambda(lambda x: K.repeat_elements(x, mc_iteration, axis=0))(inputs2)\n",
    "    logits = model(mc_samples)\n",
    "    ret_shape = model.layers[-1].output_shape\n",
    "    ret_shape = (-1, mc_iteration, *ret_shape[1:])\n",
    "    probs = Lambda(lambda x: K.reshape(x, ret_shape))(logits)\n",
    "    outputs = Lambda(lambda x: K.mean(x, axis=1))(probs)\n",
    "    model2 = Model(inputs=inputs2, outputs=[outputs,outputs])\n",
    "\n",
    "    #model2.compile(optimizer=Adam(lr=1e-6), loss=original2, metrics=['accuracy'])\n",
    "    #model2.compile(optimizer=Adam(lr=1e-6), loss={'lambda_2':'categorical_crossentropy','lambda_2_1':original},loss_weights={'lambda_2':1,'lambda_2_1':8},metrics=[])\n",
    "    model2.compile(optimizer=Adam(lr=1e-6), loss={'lambda_2':'categorical_crossentropy','lambda_2_1':original},metrics=[])\n",
    "    \n",
    "    \n",
    "    train=\"/home/sora/new_project/crop/dataset_%d/train/\"%data_num\n",
    "    val=\"/home/sora/new_project/crop/dataset_%d/val/\"%data_num\n",
    "    # train -----------------------------------------------------------\n",
    "    train_gen = newgenerator.ImageSequence2(train,batch_size,\"train\",data_num)\n",
    "    valid_gen = newgenerator.ImageSequence2(val, batch_size,\"val\",data_num)\n",
    "    os.makedirs(save_path + 'weights2', exist_ok=True)\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(save_path,'weights2', 'weights.h5'),\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "    history = model2.fit_generator(generator=train_gen,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_gen),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint],\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=len(valid_gen))\n",
    "\n",
    "    tl.draw_train_loss_plot(history, save_path)\n",
    "    #draw_train_loss_plot2(history, save_path)\n",
    "    #draw_train_loss_plot3(history, save_path)\n",
    "    #draw_train_loss_plot4(history, save_path)\n",
    "    del model,model2,input_shape,outputs,probs,mc_samples,logits,ret_shape,history,train_gen,valid_gen,model_checkpoint,physical_devices,save_path2,save_path\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n",
      "memory growth: True\n",
      "WARNING:tensorflow:From <ipython-input-8-5236f758ce71>:55: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sora/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Training option is ignored..\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05465, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 925s 351ms/step - loss: 0.0317 - accuracy: 0.9500 - val_loss: 0.0547 - val_accuracy: 0.9098\n",
      "Epoch 2/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9508\n",
      "Epoch 00002: val_loss improved from 0.05465 to 0.05292, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 306s 116ms/step - loss: 0.0290 - accuracy: 0.9508 - val_loss: 0.0529 - val_accuracy: 0.9090\n",
      "Epoch 3/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9513\n",
      "Epoch 00003: val_loss improved from 0.05292 to 0.05215, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 306s 116ms/step - loss: 0.0280 - accuracy: 0.9513 - val_loss: 0.0522 - val_accuracy: 0.9086\n",
      "Epoch 4/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9519\n",
      "Epoch 00004: val_loss improved from 0.05215 to 0.05211, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 307s 116ms/step - loss: 0.0273 - accuracy: 0.9519 - val_loss: 0.0521 - val_accuracy: 0.9080\n",
      "Epoch 5/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9522\n",
      "Epoch 00005: val_loss improved from 0.05211 to 0.05188, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 306s 116ms/step - loss: 0.0270 - accuracy: 0.9522 - val_loss: 0.0519 - val_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9529\n",
      "Epoch 00006: val_loss improved from 0.05188 to 0.05138, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 307s 116ms/step - loss: 0.0265 - accuracy: 0.9529 - val_loss: 0.0514 - val_accuracy: 0.9087\n",
      "Epoch 7/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9532\n",
      "Epoch 00007: val_loss did not improve from 0.05138\n",
      "2636/2636 [==============================] - 305s 116ms/step - loss: 0.0263 - accuracy: 0.9532 - val_loss: 0.0517 - val_accuracy: 0.9076\n",
      "Epoch 8/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9533\n",
      "Epoch 00008: val_loss did not improve from 0.05138\n",
      "2636/2636 [==============================] - 305s 116ms/step - loss: 0.0262 - accuracy: 0.9533 - val_loss: 0.0515 - val_accuracy: 0.9080\n",
      "Epoch 9/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9538\n",
      "Epoch 00009: val_loss did not improve from 0.05138\n",
      "2636/2636 [==============================] - 305s 116ms/step - loss: 0.0259 - accuracy: 0.9538 - val_loss: 0.0514 - val_accuracy: 0.9082\n",
      "Epoch 10/10\n",
      "2636/2636 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9541\n",
      "Epoch 00010: val_loss improved from 0.05138 to 0.05121, saving model to ./ips/unet_patch/unet/size_160/dataset_1/stage2/weights/weights.h5\n",
      "2636/2636 [==============================] - 306s 116ms/step - loss: 0.0257 - accuracy: 0.9541 - val_loss: 0.0512 - val_accuracy: 0.9084\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9428\n",
      "Epoch 00001: val_loss improved from inf to 0.06595, saving model to ./ips/unet_patch/unet/size_160/dataset_2/stage2/weights/weights.h5\n",
      "2833/2833 [==============================] - 946s 334ms/step - loss: 0.0322 - accuracy: 0.9428 - val_loss: 0.0659 - val_accuracy: 0.8835\n",
      "Epoch 2/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9442\n",
      "Epoch 00002: val_loss did not improve from 0.06595\n",
      "2833/2833 [==============================] - 322s 114ms/step - loss: 0.0309 - accuracy: 0.9442 - val_loss: 0.0660 - val_accuracy: 0.8823\n",
      "Epoch 3/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9453\n",
      "Epoch 00003: val_loss did not improve from 0.06595\n",
      "2833/2833 [==============================] - 323s 114ms/step - loss: 0.0302 - accuracy: 0.9453 - val_loss: 0.0665 - val_accuracy: 0.8813\n",
      "Epoch 4/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9462\n",
      "Epoch 00004: val_loss improved from 0.06595 to 0.06578, saving model to ./ips/unet_patch/unet/size_160/dataset_2/stage2/weights/weights.h5\n",
      "2833/2833 [==============================] - 325s 115ms/step - loss: 0.0297 - accuracy: 0.9462 - val_loss: 0.0658 - val_accuracy: 0.8830\n",
      "Epoch 5/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9468\n",
      "Epoch 00005: val_loss improved from 0.06578 to 0.06473, saving model to ./ips/unet_patch/unet/size_160/dataset_2/stage2/weights/weights.h5\n",
      "2833/2833 [==============================] - 324s 114ms/step - loss: 0.0294 - accuracy: 0.9468 - val_loss: 0.0647 - val_accuracy: 0.8850\n",
      "Epoch 6/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9473\n",
      "Epoch 00006: val_loss did not improve from 0.06473\n",
      "2833/2833 [==============================] - 323s 114ms/step - loss: 0.0290 - accuracy: 0.9473 - val_loss: 0.0674 - val_accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9477\n",
      "Epoch 00007: val_loss did not improve from 0.06473\n",
      "2833/2833 [==============================] - 323s 114ms/step - loss: 0.0288 - accuracy: 0.9477 - val_loss: 0.0661 - val_accuracy: 0.8828\n",
      "Epoch 8/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9484\n",
      "Epoch 00008: val_loss did not improve from 0.06473\n",
      "2833/2833 [==============================] - 323s 114ms/step - loss: 0.0285 - accuracy: 0.9484 - val_loss: 0.0669 - val_accuracy: 0.8815\n",
      "Epoch 9/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9489\n",
      "Epoch 00009: val_loss did not improve from 0.06473\n",
      "2833/2833 [==============================] - 322s 114ms/step - loss: 0.0283 - accuracy: 0.9489 - val_loss: 0.0665 - val_accuracy: 0.8823\n",
      "Epoch 10/10\n",
      "2833/2833 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9494\n",
      "Epoch 00010: val_loss did not improve from 0.06473\n",
      "2833/2833 [==============================] - 322s 114ms/step - loss: 0.0280 - accuracy: 0.9494 - val_loss: 0.0668 - val_accuracy: 0.8815\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9336\n",
      "Epoch 00001: val_loss improved from inf to 0.04215, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 998s 342ms/step - loss: 0.0381 - accuracy: 0.9336 - val_loss: 0.0421 - val_accuracy: 0.9270\n",
      "Epoch 2/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9347\n",
      "Epoch 00002: val_loss improved from 0.04215 to 0.04159, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 641s 220ms/step - loss: 0.0364 - accuracy: 0.9347 - val_loss: 0.0416 - val_accuracy: 0.9268\n",
      "Epoch 3/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9355\n",
      "Epoch 00003: val_loss improved from 0.04159 to 0.04102, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 337s 115ms/step - loss: 0.0356 - accuracy: 0.9355 - val_loss: 0.0410 - val_accuracy: 0.9271\n",
      "Epoch 4/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9364\n",
      "Epoch 00004: val_loss improved from 0.04102 to 0.04083, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 337s 116ms/step - loss: 0.0350 - accuracy: 0.9364 - val_loss: 0.0408 - val_accuracy: 0.9271\n",
      "Epoch 5/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9371\n",
      "Epoch 00005: val_loss did not improve from 0.04083\n",
      "2918/2918 [==============================] - 335s 115ms/step - loss: 0.0347 - accuracy: 0.9371 - val_loss: 0.0409 - val_accuracy: 0.9267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9380\n",
      "Epoch 00006: val_loss improved from 0.04083 to 0.04076, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 339s 116ms/step - loss: 0.0342 - accuracy: 0.9380 - val_loss: 0.0408 - val_accuracy: 0.9273\n",
      "Epoch 7/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9385\n",
      "Epoch 00007: val_loss did not improve from 0.04076\n",
      "2918/2918 [==============================] - 337s 116ms/step - loss: 0.0339 - accuracy: 0.9385 - val_loss: 0.0412 - val_accuracy: 0.9263\n",
      "Epoch 8/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9395\n",
      "Epoch 00008: val_loss improved from 0.04076 to 0.04067, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 339s 116ms/step - loss: 0.0334 - accuracy: 0.9395 - val_loss: 0.0407 - val_accuracy: 0.9272\n",
      "Epoch 9/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9399\n",
      "Epoch 00009: val_loss improved from 0.04067 to 0.04009, saving model to ./ips/unet_patch/unet/size_160/dataset_3/stage2/weights/weights.h5\n",
      "2918/2918 [==============================] - 339s 116ms/step - loss: 0.0332 - accuracy: 0.9399 - val_loss: 0.0401 - val_accuracy: 0.9281\n",
      "Epoch 10/10\n",
      "2918/2918 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9406\n",
      "Epoch 00010: val_loss did not improve from 0.04009\n",
      "2918/2918 [==============================] - 336s 115ms/step - loss: 0.0328 - accuracy: 0.9406 - val_loss: 0.0407 - val_accuracy: 0.9272\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9016\n",
      "Epoch 00001: val_loss improved from inf to 0.03788, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights/weights.h5\n",
      "3052/3052 [==============================] - 1237s 405ms/step - loss: 0.0538 - accuracy: 0.9016 - val_loss: 0.0379 - val_accuracy: 0.9336\n",
      "Epoch 2/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9063\n",
      "Epoch 00002: val_loss improved from 0.03788 to 0.03710, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights/weights.h5\n",
      "3052/3052 [==============================] - 560s 183ms/step - loss: 0.0507 - accuracy: 0.9063 - val_loss: 0.0371 - val_accuracy: 0.9340\n",
      "Epoch 3/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9088\n",
      "Epoch 00003: val_loss improved from 0.03710 to 0.03605, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights/weights.h5\n",
      "3052/3052 [==============================] - 353s 116ms/step - loss: 0.0494 - accuracy: 0.9088 - val_loss: 0.0360 - val_accuracy: 0.9354\n",
      "Epoch 4/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9109\n",
      "Epoch 00004: val_loss improved from 0.03605 to 0.03590, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights/weights.h5\n",
      "3052/3052 [==============================] - 352s 115ms/step - loss: 0.0483 - accuracy: 0.9109 - val_loss: 0.0359 - val_accuracy: 0.9355\n",
      "Epoch 5/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9128\n",
      "Epoch 00005: val_loss did not improve from 0.03590\n",
      "3052/3052 [==============================] - 352s 115ms/step - loss: 0.0474 - accuracy: 0.9128 - val_loss: 0.0366 - val_accuracy: 0.9342\n",
      "Epoch 6/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9146\n",
      "Epoch 00006: val_loss did not improve from 0.03590\n",
      "3052/3052 [==============================] - 351s 115ms/step - loss: 0.0465 - accuracy: 0.9146 - val_loss: 0.0361 - val_accuracy: 0.9348\n",
      "Epoch 7/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9158\n",
      "Epoch 00007: val_loss did not improve from 0.03590\n",
      "3052/3052 [==============================] - 351s 115ms/step - loss: 0.0459 - accuracy: 0.9158 - val_loss: 0.0361 - val_accuracy: 0.9349\n",
      "Epoch 8/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9171\n",
      "Epoch 00008: val_loss did not improve from 0.03590\n",
      "3052/3052 [==============================] - 352s 115ms/step - loss: 0.0452 - accuracy: 0.9171 - val_loss: 0.0369 - val_accuracy: 0.9332\n",
      "Epoch 9/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9184\n",
      "Epoch 00009: val_loss did not improve from 0.03590\n",
      "3052/3052 [==============================] - 352s 115ms/step - loss: 0.0446 - accuracy: 0.9184 - val_loss: 0.0363 - val_accuracy: 0.9346\n",
      "Epoch 10/10\n",
      "3052/3052 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9198\n",
      "Epoch 00010: val_loss improved from 0.03590 to 0.03587, saving model to ./ips/unet_patch/unet/size_160/dataset_4/stage2/weights/weights.h5\n",
      "3052/3052 [==============================] - 353s 116ms/step - loss: 0.0439 - accuracy: 0.9198 - val_loss: 0.0359 - val_accuracy: 0.9351\n",
      "memory growth: True\n",
      "memory growth: True\n",
      "Epoch 1/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9243\n",
      "Epoch 00001: val_loss improved from inf to 0.04241, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights/weights.h5\n",
      "2806/2806 [==============================] - 914s 326ms/step - loss: 0.0420 - accuracy: 0.9243 - val_loss: 0.0424 - val_accuracy: 0.9252\n",
      "Epoch 2/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9256\n",
      "Epoch 00002: val_loss improved from 0.04241 to 0.04150, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights/weights.h5\n",
      "2806/2806 [==============================] - 500s 178ms/step - loss: 0.0404 - accuracy: 0.9256 - val_loss: 0.0415 - val_accuracy: 0.9268\n",
      "Epoch 3/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9267\n",
      "Epoch 00003: val_loss improved from 0.04150 to 0.04138, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights/weights.h5\n",
      "2806/2806 [==============================] - 327s 116ms/step - loss: 0.0397 - accuracy: 0.9267 - val_loss: 0.0414 - val_accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9279\n",
      "Epoch 00004: val_loss improved from 0.04138 to 0.04081, saving model to ./ips/unet_patch/unet/size_160/dataset_5/stage2/weights/weights.h5\n",
      "2806/2806 [==============================] - 327s 117ms/step - loss: 0.0391 - accuracy: 0.9279 - val_loss: 0.0408 - val_accuracy: 0.9282\n",
      "Epoch 5/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9286\n",
      "Epoch 00005: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 325s 116ms/step - loss: 0.0387 - accuracy: 0.9286 - val_loss: 0.0418 - val_accuracy: 0.9264\n",
      "Epoch 6/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9296\n",
      "Epoch 00006: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 326s 116ms/step - loss: 0.0382 - accuracy: 0.9296 - val_loss: 0.0425 - val_accuracy: 0.9255\n",
      "Epoch 7/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9304\n",
      "Epoch 00007: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 325s 116ms/step - loss: 0.0378 - accuracy: 0.9304 - val_loss: 0.0417 - val_accuracy: 0.9268\n",
      "Epoch 8/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9312\n",
      "Epoch 00008: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 325s 116ms/step - loss: 0.0374 - accuracy: 0.9312 - val_loss: 0.0413 - val_accuracy: 0.9276\n",
      "Epoch 9/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9319\n",
      "Epoch 00009: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 325s 116ms/step - loss: 0.0371 - accuracy: 0.9319 - val_loss: 0.0413 - val_accuracy: 0.9275\n",
      "Epoch 10/10\n",
      "2806/2806 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9327\n",
      "Epoch 00010: val_loss did not improve from 0.04081\n",
      "2806/2806 [==============================] - 325s 116ms/step - loss: 0.0367 - accuracy: 0.9327 - val_loss: 0.0424 - val_accuracy: 0.9261\n"
     ]
    }
   ],
   "source": [
    "#自作lossでCurriculum\n",
    "for data_num in [1,2,3,4,5]:\n",
    "    clear_session()\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        for k in range(len(physical_devices)):\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[k], True)\n",
    "            print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[k]))\n",
    "    else:\n",
    "        print(\"Not enough GPU hardware devices available\")\n",
    "        \n",
    "    save_path = tl.get_save_path(data, method, model_name,patch_size, data_num)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 12\n",
    "    \n",
    "    model = BayesianUNet2D((patch_size, patch_size, 3), output_channel).build()\n",
    "    \n",
    "    input_shape = (patch_size,patch_size,output_channel)\n",
    "    inputs2 = Input(input_shape)\n",
    "    mc_samples = Lambda(lambda x: K.repeat_elements(x, mc_iteration, axis=0))(inputs2)\n",
    "    logits = model(mc_samples)\n",
    "    ret_shape = model.layers[-1].output_shape\n",
    "    ret_shape = (-1, mc_iteration, *ret_shape[1:])\n",
    "    probs = Lambda(lambda x: K.reshape(x, ret_shape))(logits)\n",
    "    outputs = Lambda(lambda x: K.mean(x, axis=1))(probs)\n",
    "    model2 = Model(inputs=inputs2, outputs=[outputs,outputs])\n",
    "\n",
    "    save_path2 = save_path######\n",
    "    save_path = save_path+\"stage2/\"########\n",
    "    model2.load_weights(save_path2+'weights2/weights.h5')\n",
    "    model.compile(optimizer=Adam(lr=1e-6), loss=sensitivity_specificity, metrics=['accuracy'])\n",
    "    #model2.compile(optimizer=Adam(lr=1e-6), loss={'lambda_2':'categorical_crossentropy','lambda_2_1':original},loss_weights={'lambda_2':1,'lambda_2_1':8},metrics=[])\n",
    "    #model2.compile(optimizer=Adam(lr=1e-6), loss={'lambda_2':'categorical_crossentropy','lambda_2_1':original},metrics=[])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    train=\"/home/sora/new_project/crop/dataset_%d/train/\"%data_num\n",
    "    val=\"/home/sora/new_project/crop/dataset_%d/val/\"%data_num\n",
    "    # train -----------------------------------------------------------\n",
    "    train_gen = newgenerator.ImageSequence(train,batch_size,\"train\",data_num)\n",
    "    valid_gen = newgenerator.ImageSequence(val, batch_size,\"val\",data_num)\n",
    "    os.makedirs(save_path + 'weights', exist_ok=True)\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(save_path,'weights', 'weights.h5'),\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "    history = model.fit_generator(generator=train_gen,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_gen),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint],\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=len(valid_gen))\n",
    "\n",
    "    tl.draw_train_loss_plot(history, save_path)\n",
    "    #draw_train_loss_plot2(history, save_path)\n",
    "    #draw_train_loss_plot3(history, save_path)\n",
    "    #draw_train_loss_plot4(history, save_path)\n",
    "    #del model,model2,input_shape,outputs,probs,mc_samples,logits,ret_shape,history,train_gen,valid_gen,model_checkpoint,physical_devices,save_path2,save_path\n",
    "    del model,history,train_gen,valid_gen,model_checkpoint,physical_devices,save_path2,save_path    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"test\",\"train\"] :\n",
    "    for data_num in [1,2,3,4,5]:\n",
    "        save_path = tl.get_save_path(data, method, 'unet',patch_size, data_num)\n",
    "        os.makedirs(save_path + '%s_un/uncertainty'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/uncertainty_T_0.5'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"%s\"%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"label\", exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/correctness'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + '%s_un/probability'%mode, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"test\"] :\n",
    "    for data_num in [1]:\n",
    "        save_path = tl.get_save_path(data, method, 'unet',patch_size, data_num)\n",
    "        os.makedirs(save_path + 'stage2/%s_un/uncertainty'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + 'stage2/%s_un/uncertainty_T_0.5'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"stage2/%s\"%mode, exist_ok=True)\n",
    "        os.makedirs(save_path+\"stage2/label\", exist_ok=True)\n",
    "        os.makedirs(save_path + 'stage2/%s_un/correctness'%mode, exist_ok=True)\n",
    "        os.makedirs(save_path + 'stage2/%s_un/probability'%mode, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "data/ips/img/E3230662.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sora/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Training option is ignored..\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230715.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230729.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230705.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230734.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230709.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230730.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230667.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230678.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230665.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230668.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230692.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230708.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230652.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230719.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230707.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230704.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230679.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230698.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230693.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230666.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230685.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230688.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230661.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230716.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230727.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230656.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230733.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230713.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230722.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230669.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230699.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230696.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230695.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230681.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230735.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230657.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230724.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230659.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230731.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230712.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230714.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230700.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230701.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230689.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230683.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n",
      "************************************\n",
      "data/ips/img/E3230664.JPG\n",
      "Predict finish\n",
      "Start visualize image...\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "#予測マップ、uncertainty map作成\n",
    "height=1200\n",
    "width=1600\n",
    "step=10\n",
    "for data_num in [2,3,4,5]:\n",
    "    clear_session()\n",
    "    save_path = tl.get_save_path(data, method, 'unet',patch_size, data_num)\n",
    "    save_path = save_path\n",
    "    \n",
    "    model = BayesianUNet2D((patch_size, patch_size, 3), 3).build()\n",
    "    input_shape = (patch_size,patch_size,output_channel)\n",
    "    inputs2 = Input(input_shape)\n",
    "    mc_samples = Lambda(lambda x: K.repeat_elements(x, mc_iteration, axis=0))(inputs2)\n",
    "    logits = model(mc_samples)\n",
    "    ret_shape = model.layers[-1].output_shape\n",
    "    ret_shape = (-1, mc_iteration, *ret_shape[1:])\n",
    "    probs = Lambda(lambda x: K.reshape(x, ret_shape))(logits)\n",
    "    outputs = Lambda(lambda x: K.mean(x, axis=1))(probs)\n",
    "    model2 = Model(inputs=inputs2, outputs=[outputs,outputs])\n",
    "    \n",
    "    save_path = save_path+\"stage2/\"\n",
    "    model2.load_weights(save_path+'weights2/weights.h5')\n",
    "    \n",
    "    for mode in [\"test\"]:\n",
    "        img_path, mask_path = tl.data_path_load(data,mode,data_num)\n",
    "        for i_path in img_path:\n",
    "            print(\"************************************\")\n",
    "            print(i_path)\n",
    "            img = Image.open(i_path)\n",
    "            img = np.array(img)\n",
    "            name = os.path.basename(i_path)[:-4]\n",
    "            Yp=[]\n",
    "            y=0\n",
    "            while y < (height - patch_size + 1):\n",
    "                x = 0\n",
    "                while x < (1600 - patch_size + 1):\n",
    "                    crop_img = img[y:patch_size+y, x:patch_size+x]\n",
    "                    crop_img = crop_img/255.\n",
    "                    \n",
    "                    tmp,_ = model2.predict(crop_img[np.newaxis, :, :, :])\n",
    "                    tmp = np.reshape(tmp, (patch_size,patch_size,n_class))\n",
    "                    Yp.append(tmp)\n",
    "                    del crop_img,tmp,_\n",
    "                    gc.collect()\n",
    "                    x+=step   \n",
    "                y+=step\n",
    "            Yp = np.array(Yp)\n",
    "            \n",
    "            print(\"Predict finish\")\n",
    "            vis_img, label_img,e,pro = make_prediction_map(patch_size,\n",
    "                                                             step,\n",
    "                                                             width,\n",
    "                                                             height,\n",
    "                                                             Yp,\n",
    "                                                             reflect,\n",
    "                                                             data,\n",
    "                                                             n_class)\n",
    "            vis_img = vis_img[:, :, [2, 1, 0]]\n",
    "            pro = pro[:, :, [2, 1, 0]]\n",
    "            cv2.imwrite(save_path + '%s/' % mode+name + '.png', vis_img)\n",
    "            cv2.imwrite(save_path + '%s_un/probability/' % mode+name + '.png', pro)\n",
    "            if mode == \"test\":\n",
    "                convert2label(name, save_path)\n",
    "            tmp=e\n",
    "            e_1 = e*255\n",
    "            e_1 = Image.fromarray(e_1)\n",
    "            e_1 = e_1.convert('RGB')\n",
    "            e_1.save(save_path + '%s_un/uncertainty/'%mode + name + '.png')\n",
    "            e[e>=0.5]=255\n",
    "            e[e<0.5]=0\n",
    "            e=Image.fromarray(e)\n",
    "            e = e.convert('RGB')\n",
    "            e.save(save_path + '%s_un/uncertainty_T_0.5/'%mode + name + '.png')\n",
    "            del Yp,vis_img, label_img,e,e_1\n",
    "            gc.collect()\n",
    "            print(\"************************************\")\n",
    "    del model,model2,input_shape,outputs,probs,mc_samples,logits,ret_shape\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_data_path_load(path):\n",
    "    img_path=[]\n",
    "    if data in ['ips', 'melanoma']:\n",
    "        for x in os.listdir(path):\n",
    "            img_path.append(path+x)\n",
    "        return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ips/mask/E3230662.png\n",
      "data/ips/mask/E3230665.png\n",
      "data/ips/mask/E3230667.png\n",
      "data/ips/mask/E3230668.png\n",
      "data/ips/mask/E3230678.png\n",
      "data/ips/mask/E3230692.png\n",
      "data/ips/mask/E3230705.png\n",
      "data/ips/mask/E3230709.png\n",
      "data/ips/mask/E3230715.png\n",
      "data/ips/mask/E3230729.png\n",
      "data/ips/mask/E3230730.png\n",
      "data/ips/mask/E3230734.png\n",
      "data/ips/mask/E3230652.png\n",
      "data/ips/mask/E3230666.png\n",
      "data/ips/mask/E3230679.png\n",
      "data/ips/mask/E3230685.png\n",
      "data/ips/mask/E3230688.png\n",
      "data/ips/mask/E3230693.png\n",
      "data/ips/mask/E3230698.png\n",
      "data/ips/mask/E3230704.png\n",
      "data/ips/mask/E3230707.png\n",
      "data/ips/mask/E3230708.png\n",
      "data/ips/mask/E3230719.png\n",
      "data/ips/mask/E3230656.png\n",
      "data/ips/mask/E3230661.png\n",
      "data/ips/mask/E3230669.png\n",
      "data/ips/mask/E3230681.png\n",
      "data/ips/mask/E3230695.png\n",
      "data/ips/mask/E3230696.png\n",
      "data/ips/mask/E3230699.png\n",
      "data/ips/mask/E3230713.png\n",
      "data/ips/mask/E3230716.png\n",
      "data/ips/mask/E3230722.png\n",
      "data/ips/mask/E3230727.png\n",
      "data/ips/mask/E3230733.png\n",
      "data/ips/mask/E3230657.png\n",
      "data/ips/mask/E3230659.png\n",
      "data/ips/mask/E3230664.png\n",
      "data/ips/mask/E3230683.png\n",
      "data/ips/mask/E3230689.png\n",
      "data/ips/mask/E3230700.png\n",
      "data/ips/mask/E3230701.png\n",
      "data/ips/mask/E3230712.png\n",
      "data/ips/mask/E3230714.png\n",
      "data/ips/mask/E3230724.png\n",
      "data/ips/mask/E3230731.png\n",
      "data/ips/mask/E3230735.png\n"
     ]
    }
   ],
   "source": [
    "#correctness map作成\n",
    "for data_num in [2,3,4,5]:\n",
    "    img_path, mask_path = tl.data_path_load(data,'test',data_num)\n",
    "    mode=\"test\"\n",
    "    model_name=\"unet\"\n",
    "    size=160\n",
    "    #path=(\"./%s/%s/%s/size_%d/dataset_%d/\" %(data, method,model_name,size,data_num))\n",
    "    path=(\"./%s/%s/%s/size_%d/dataset_%d/stage2/\" %(data, method,model_name,size,data_num))\n",
    "    #path=(\"./%s/%s/%s/size_%d/dataset_%d/stage2/stage3/stage4/\" %(data, method,model_name,size,data_num))\n",
    "\n",
    "    pre_path = pre_data_path_load(path+\"%s/\"%mode)\n",
    "\n",
    "    mask_path.sort()\n",
    "    pre_path.sort()\n",
    "    for m_path,p_path in zip(mask_path,pre_path):\n",
    "        print(m_path)\n",
    "        name = os.path.basename(m_path)[:-4]\n",
    "        m_mask = cv2.imread(m_path)\n",
    "        m_mask = cv2.cvtColor(m_mask, cv2.COLOR_BGR2RGB)\n",
    "        height, width = m_mask.shape[:2]\n",
    "        imim = m_mask[:,:,0]+m_mask[:,:,1]+m_mask[:,:,2]\n",
    "        m_mask[:,:,0]=(imim==255)*m_mask[:,:,0]\n",
    "        m_mask[:,:,1]=(imim==255)*m_mask[:,:,1]\n",
    "        m_mask[:,:,2]=(imim==255)*m_mask[:,:,2]\n",
    "\n",
    "        mask = get_ytrue(m_mask, 'ips')\n",
    "        oor = ~(mask == 0) *1\n",
    "\n",
    "        p_image = cv2.imread(p_path)\n",
    "        p_image = cv2.cvtColor(p_image, cv2.COLOR_BGR2RGB)\n",
    "        p_image[:,:,0] = p_image[:,:,0]* oor \n",
    "        p_image[:,:,1] = p_image[:,:,1] * oor \n",
    "        p_image[:,:,2] = p_image[:,:,2] * oor\n",
    "        pre = get_ytrue(p_image, 'ips')\n",
    "        true=mask-pre\n",
    "        true[true!=0]=255\n",
    "        true=true*oor\n",
    "        true=Image.fromarray(true)\n",
    "        true = true.convert('RGB')\n",
    "        true.save(path + 'test_un/correctness/' + name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
